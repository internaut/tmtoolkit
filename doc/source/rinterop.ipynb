{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f3f426f",
   "metadata": {},
   "source": [
    "# Interoperability with R\n",
    "\n",
    "There are many popular R packages for text mining, topic modeling and NLP like [tm](https://cran.r-project.org/web/packages/tm/index.html) or [topicmodels](https://cran.r-project.org/web/packages/topicmodels/index.html). If for some reason you need to implement parts of your work in Python with tmtoolkit and other parts in R, you can do that quite easily.\n",
    "\n",
    "First of all, you can import and export all tabular data to and from Python using tabular data formats like CSV or Excel. See for example the sections on [tabular tokens output](preprocessing.ipynb#Accessing-tokens-and-token-attributes) or [exporting topic modeling results](topic_modeling.ipynb#Displaying-and-exporting-topic-modeling-results) and check out the [load_corpus_from_tokens_table](api.rst#tmtoolkit.corpus.load_corpus_from_tokens_table) function.\n",
    "\n",
    "However, if you only want to load a document-term matrix (DTM) that you generated with tmtoolkit into R or vice versa, the most efficient way is to store this matrix along with all necessary metadata to an [RDS file](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/readRDS) as explained in the following section. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**\n",
    "\n",
    "You will need to install tmtoolkit with the \"rinterop\" option in order to use the functions explained in this chapter: `pip install tmtoolkit[rinterop]`. This is only available since version 0.12.0.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## Saving a (sparse) document-term matrix to an RDS file\n",
    "\n",
    "A common scenario is that you used tmtoolkit for preprocessing your text corpus and generated a DTM along with document labels and the corpus vocabulary. For further processing you want to use R, e.g. for topic modeling with the *topicmodels* package. You can do so by using the [save_dtm_to_rds](api.rst#tmtoolkit.bow.dtm.save_dtm_to_rds) function.\n",
    "\n",
    "First, we generate a DTM from some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a7af48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:04.552430Z",
     "iopub.status.busy": "2023-03-31T14:02:04.551627Z",
     "iopub.status.idle": "2023-03-31T14:02:08.650418Z",
     "shell.execute_reply": "2023-03-31T14:02:08.649821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 10 documents in English\n",
      "> News100-1771 (522 tokens): Trump invited Navy SEAL widow Carryn Owens to join...\n",
      "> News100-3539 (1624 tokens): Ana 's Fate Rested With An Asylum Officer Who Had ...\n",
      "> News100-3031 (1440 tokens): Battle over Ireland 's last Magdalene laundry    I...\n",
      "> News100-368 (578 tokens): Air pollution concerns potential overseas talent  ...\n",
      "> News100-2515 (167 tokens): Four killed in Austrian avalanche    Four Swiss me...\n",
      "> News100-2483 (840 tokens): UN report : Israel has established an ' apartheid ...\n",
      "> News100-895 (1378 tokens): German retailer KiK compensates Pakistan 's ' indu...\n",
      "> News100-3228 (599 tokens): Neil Gorsuch facing ' rigorous ' confirmation hear...\n",
      "> News100-1813 (1743 tokens): Press review : Russia changes anti - doping tune a...\n",
      "> News100-2787 (1184 tokens): French architect Le Corbusier 's foray into the Fa...\n",
      "total number of tokens: 10075 / vocabulary size: 2759\n"
     ]
    }
   ],
   "source": [
    "import tmtoolkit.corpus as c\n",
    "\n",
    "corp = c.Corpus.from_builtin_corpus('en-News100', sample=10)\n",
    "c.print_summary(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff0554b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.653003Z",
     "iopub.status.busy": "2023-03-31T14:02:08.652595Z",
     "iopub.status.idle": "2023-03-31T14:02:08.691192Z",
     "shell.execute_reply": "2023-03-31T14:02:08.690614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 10 documents in English\n",
      "> News100-1771 (122 tokens): trump invite white house wednesday share detail su...\n",
      "> News100-3539 (355 tokens): rest officer tell doubt word bear head trump admin...\n",
      "> News100-3031 (331 tokens): ireland ireland sale want memorial woman abuse han...\n",
      "> News100-368 (138 tokens): air potential percent foreign worker problem air u...\n",
      "> News100-2515 (41 tokens): kill swiss man kill group away western austria pol...\n",
      "> News100-2483 (188 tokens): report establish white report break new ground sit...\n",
      "> News100-895 (330 tokens): industrial family company release $ compensation k...\n",
      "> News100-3228 (138 tokens): face hearing week president donald trump justice c...\n",
      "> News100-1813 (376 tokens): press change language story press thursday march f...\n",
      "> News100-2787 (218 tokens): east national western world summer good man job bu...\n",
      "total number of tokens: 2237 / vocabulary size: 494\n"
     ]
    }
   ],
   "source": [
    "c.lemmatize(corp)\n",
    "c.to_lowercase(corp)\n",
    "c.filter_clean_tokens(corp, remove_numbers=True)\n",
    "c.remove_common_tokens(corp, df_threshold=0.9)\n",
    "c.remove_uncommon_tokens(corp, df_threshold=0.1)\n",
    "\n",
    "c.print_summary(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2fd48e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.693474Z",
     "iopub.status.busy": "2023-03-31T14:02:08.693297Z",
     "iopub.status.idle": "2023-03-31T14:02:08.701057Z",
     "shell.execute_reply": "2023-03-31T14:02:08.700476Z"
    }
   },
   "outputs": [],
   "source": [
    "dtm, doc_labels, vocab = c.dtm(corp, return_doc_labels=True, return_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16429d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.703192Z",
     "iopub.status.busy": "2023-03-31T14:02:08.703006Z",
     "iopub.status.idle": "2023-03-31T14:02:08.706768Z",
     "shell.execute_reply": "2023-03-31T14:02:08.706142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 document labels:\n",
      "['News100-1771', 'News100-1813', 'News100-2483', 'News100-2515', 'News100-2787', 'News100-3031', 'News100-3228', 'News100-3539', 'News100-368', 'News100-895']\n",
      "first 10 vocabulary tokens:\n",
      "['$', 'able', 'abuse', 'accept', 'access', 'accord', 'account', 'accuse', 'acknowledge', 'act']\n",
      "DTM shape:\n",
      "(10, 494)\n"
     ]
    }
   ],
   "source": [
    "print('first 10 document labels:')\n",
    "print(doc_labels[:10])\n",
    "\n",
    "print('first 10 vocabulary tokens:')\n",
    "print(vocab[:10])\n",
    "\n",
    "print('DTM shape:')\n",
    "print(dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c133b",
   "metadata": {},
   "source": [
    "The DTM is stored a sparse matrix. **It's highly recommended to use a sparse matrix representation, especially when you're working with large text corpora.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8d8237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.709011Z",
     "iopub.status.busy": "2023-03-31T14:02:08.708840Z",
     "iopub.status.idle": "2023-03-31T14:02:08.716189Z",
     "shell.execute_reply": "2023-03-31T14:02:08.715769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x494 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 1348 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd5feb",
   "metadata": {},
   "source": [
    "Now, we save the DTM along with the document labels and the vocabulary as sparse matrix to an RDS file, that we can load into R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba37f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.718473Z",
     "iopub.status.busy": "2023-03-31T14:02:08.718094Z",
     "iopub.status.idle": "2023-03-31T14:02:08.740162Z",
     "shell.execute_reply": "2023-03-31T14:02:08.739637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving DTM, document labels and vocabulary to file \"data/dtm.RDS\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tmtoolkit.bow.dtm import save_dtm_to_rds\n",
    "\n",
    "rds_file = os.path.join('data', 'dtm.RDS')\n",
    "print(f'saving DTM, document labels and vocabulary to file \"{rds_file}\"')\n",
    "save_dtm_to_rds(rds_file, dtm, doc_labels, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfcf6e3",
   "metadata": {},
   "source": [
    "The following R code would load this DTM from the RDS file and fit a topic model via LDA with 20 topics:\n",
    "\n",
    "```R\n",
    "library(Matrix)       # for sparseMatrix in RDS file\n",
    "library(topicmodels)  # for LDA()\n",
    "library(slam)         # for as.simple_triplet_matrix()\n",
    "\n",
    "# load data \n",
    "dtm <- readRDS('data/dtm.RDS')\n",
    "class(dtm)\n",
    "dtm  # sparse matrix with document labels as row names, vocabulary as column names\n",
    "\n",
    "# convert sparse matrix to triplet format required for LDA\n",
    "dtm <- as.simple_triplet_matrix(dtm)\n",
    "\n",
    "# fit a topic model\n",
    "topicmodel <- LDA(dtm, k = 20, method = 'Gibbs')\n",
    "\n",
    "# investigate the topics\n",
    "terms(topicmodel, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba15ed6",
   "metadata": {},
   "source": [
    "## Load a (sparse) document-term matrix from an RDS file\n",
    "\n",
    "The opposite direction is also possible. For example, you may have preprocessed a text corpus in R and generated a (sparse) DTM along with its document labels and vocabulary. You can write this data to an RDS file and load it into Python/tmtoolkit. The following R code shows an example to generate a sparse DTM and store it to `data/dtm2.RDS`:\n",
    "\n",
    "```R\n",
    "library(Matrix)       # for sparseMatrix\n",
    "library(tm)           # for DocumentTermMatrix\n",
    "\n",
    "data(\"crude\")\n",
    "\n",
    "dtm <- DocumentTermMatrix(crude, control = list(removePunctuation = TRUE, stopwords = TRUE))\n",
    "\n",
    "dtm_out <- sparseMatrix(i = dtm$i, j = dtm$j, x = dtm$v, dims = dim(dtm),\n",
    "                        dimnames = dimnames(dtm))\n",
    "\n",
    "saveRDS(dtm_out, 'data/dtm2.RDS')\n",
    "```\n",
    "\n",
    "We can now load the DTM along with its document labels and vocabulary from this RDS file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8556101e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.742586Z",
     "iopub.status.busy": "2023-03-31T14:02:08.742404Z",
     "iopub.status.idle": "2023-03-31T14:02:08.750818Z",
     "shell.execute_reply": "2023-03-31T14:02:08.750244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading DTM, document labels and vocabulary from file \"data/dtm2.RDS\"\n",
      "first 10 document labels:\n",
      "['127', '144', '191', '194', '211', '236', '237', '242', '246', '248']\n",
      "first 10 vocabulary tokens:\n",
      "['100000', '108', '111', '115', '12217', '1232', '1381', '13member', '13nation', '150']\n",
      "DTM shape:\n",
      "(20, 1000)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from tmtoolkit.bow.dtm import read_dtm_from_rds\n",
    "\n",
    "\n",
    "rds_file = os.path.join('data', 'dtm2.RDS')\n",
    "print(f'loading DTM, document labels and vocabulary from file \"{rds_file}\"')\n",
    "dtm, doc_labels, vocab = read_dtm_from_rds(rds_file)\n",
    "\n",
    "print('first 10 document labels:')\n",
    "print(doc_labels[:10])\n",
    "\n",
    "print('first 10 vocabulary tokens:')\n",
    "print(vocab[:10])\n",
    "\n",
    "print('DTM shape:')\n",
    "print(dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70775411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.752902Z",
     "iopub.status.busy": "2023-03-31T14:02:08.752724Z",
     "iopub.status.idle": "2023-03-31T14:02:08.755911Z",
     "shell.execute_reply": "2023-03-31T14:02:08.755523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1738 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac19645",
   "metadata": {},
   "source": [
    "Note that the DTM was loaded as floating point matrix, but it makes more sense to represent the term frequencies as integers, since they are essentially counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab37a5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.758106Z",
     "iopub.status.busy": "2023-03-31T14:02:08.757931Z",
     "iopub.status.idle": "2023-03-31T14:02:08.761523Z",
     "shell.execute_reply": "2023-03-31T14:02:08.761120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1738 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm = dtm.astype('int')\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885364d0",
   "metadata": {},
   "source": [
    "We could now further process and analyze this DTM with tmtoolkit. For example, we can display to three most frequent tokens per document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e71a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T14:02:08.763656Z",
     "iopub.status.busy": "2023-03-31T14:02:08.763480Z",
     "iopub.status.idle": "2023-03-31T14:02:08.777012Z",
     "shell.execute_reply": "2023-03-31T14:02:08.776482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc</th>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">127</th>\n",
       "      <th>1</th>\n",
       "      <td>oil</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prices</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">144</th>\n",
       "      <th>1</th>\n",
       "      <td>opec</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oil</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">191</th>\n",
       "      <th>1</th>\n",
       "      <td>canadian</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texaco</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crude</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">194</th>\n",
       "      <th>1</th>\n",
       "      <td>crude</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>west</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">211</th>\n",
       "      <th>1</th>\n",
       "      <td>said</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estimates</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trust</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              token  value\n",
       "doc rank                  \n",
       "127 1           oil      5\n",
       "    2        prices      3\n",
       "    3          said      3\n",
       "144 1          opec     13\n",
       "    2           oil     12\n",
       "    3          said     11\n",
       "191 1      canadian      2\n",
       "    2        texaco      2\n",
       "    3         crude      2\n",
       "194 1         crude      3\n",
       "    2         price      2\n",
       "    3          west      2\n",
       "211 1          said      3\n",
       "    2     estimates      2\n",
       "    3         trust      2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tmtoolkit.bow.bow_stats import sorted_terms_table\n",
    "\n",
    "# selecting only the first 5 documents\n",
    "sorted_terms_table(dtm[:5, :], vocab=vocab, doc_labels=doc_labels[:5], top_n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
